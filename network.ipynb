{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a neural network based on the modified U-Net architecture found in the DeepHarmony paper (as pictured below). In addition, it features batch normalization layers integrated within the network and a compound loss function made up of MS-SSIM and L1. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from os import scandir\n",
    "import skimage.io as io \n",
    "import skimage.transform as trans\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.activations import *\n",
    "from keras.metrics import mean_squared_error as mse\n",
    "from keras import backend as keras \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "from mriPrep import Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Definition of the Network Class, including loss function definition and convolution definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet:\n",
    "\n",
    "    # initialization, including declaration of input sizes, coefficients for loss function\n",
    "    def __init__(self):\n",
    "        self.input_size = (128, 128, 1)\n",
    "        self.coe = [random(), random()]\n",
    "\n",
    "    # compound loss function\n",
    "    def comp_loss(self):\n",
    "        ssim = 1 - tf.reduce_mean(tf.image.ssim(self.y_true, self.y_pred, 1.0))\n",
    "        compound_loss = (self.coe[0] * ssim) + (self.coe[1] * mse(self.y_true, self.y_pred)) \n",
    "        return compound_loss\n",
    "    \n",
    "    # side convolution\n",
    "    def convolution(self, inp, n_filters):\n",
    "        conv = Conv2D(n_filters, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\")(inp)\n",
    "        bn = BatchNormalization(axis=1, momentum=0.99, epsilon=0.0001)(conv)\n",
    "        return bn\n",
    "\n",
    "    # down convolution\n",
    "    def down_convolution(self, inp, n_filters):\n",
    "        conv = Conv2D(n_filters, (4, 4), activation=\"relu\", padding=\"same\", strides=(2,2) kernel_initializer=\"he_normal\")(inp)\n",
    "        bn = bn = BatchNormalization(axis=1, momentum=0.99, epsilon=0.0001)(conv)\n",
    "        return bn\n",
    "\n",
    "    # up convolution\n",
    "    def up_convolution(self, inp, n_filters, conv_features):\n",
    "        deconvolution = Conv2DTranspose(n_filters, (4, 4), activation = 'relu', padding = 'same', strides=(0.5, 0.5), kernel_initializer = 'he_normal')(inp)\n",
    "        concatenation = Concatenate([conv_features, deconvolution], axis=3)\n",
    "        return concatenation \n",
    "\n",
    "    # definition of neural network\n",
    "    def unet(self, pretrained_weights=None):\n",
    "\n",
    "        inputs = Input(self.input_size)\n",
    "        \n",
    "        # side\n",
    "        conv1 = self.convolution(inputs, 16)\n",
    "\n",
    "        # down\n",
    "        conv2 = self.down_convolution(conv1, 16)\n",
    "        \n",
    "        # side\n",
    "        conv3 = self.convolution(conv2, 32)\n",
    "        \n",
    "        # down\n",
    "        conv4 = self.down_convolution(conv3, 32)\n",
    "\n",
    "        # side\n",
    "        conv5 = self.convolution(conv4, 64)\n",
    "\n",
    "        # down\n",
    "        conv6 = self.down_convolution(conv5, 64)\n",
    "\n",
    "        # side\n",
    "        conv7 = self.convolution(conv6, 128)\n",
    "\n",
    "        # down \n",
    "        conv8 = self.down_convolution(conv7, 128)\n",
    "        \n",
    "        # side\n",
    "        conv9 = self.convolution(conv8, 256)\n",
    "\n",
    "        # up and merge\n",
    "        conv10 = self.up_convolution(conv9, 128, conv7)\n",
    "\n",
    "        # side\n",
    "        conv11 = self.convolution(conv10, 128)\n",
    "\n",
    "        # up and merge\n",
    "        conv12 = self.up_convolution(conv11, 64, conv5)\n",
    "\n",
    "        # side \n",
    "        conv13 = self.convolution(conv12, 64)\n",
    "\n",
    "        # up and merge\n",
    "        conv14 = self.up_convolution(conv13, 32, conv3)\n",
    "\n",
    "        # side \n",
    "        conv15 = self.convolution(conv14, 32)\n",
    "\n",
    "        # up and merge\n",
    "        conv16 = self.up_convolution(conv15, 16, conv1)\n",
    "\n",
    "        # side and merge \n",
    "        conv17 = self.convolution(conv16, 16)\n",
    "        merge = Concatenate([inputs, conv17], axis=3)\n",
    "\n",
    "        # final side\n",
    "        conv18 = Conv2D(1, 1, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\")(merge)\n",
    "\n",
    "        model = Model(input = inputs, output = conv18)\n",
    "        model.compile(optimizer= Adam(lr = 1e-4),loss=self.comp_loss,metrics=[\"accuracy\"])\n",
    "        model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Data, running preprocessing programs, and Splitting into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing(\"./data/modified/\", \"./data/preprocessed/\")\n",
    "preprocessing.correctBias(\"data/modified/scanner2_sub-CC110069_T1w.nii\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 20\n",
    "model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
