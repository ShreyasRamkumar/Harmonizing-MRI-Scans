{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a neural network based on the modified U-Net architecture found in the DeepHarmony paper (as pictured below). In addition, it features batch normalization layers integrated within the network and a compound loss function made up of MS-SSIM and L1. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from os import scandir\n",
    "import skimage.io as io \n",
    "import skimage.transform as trans\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.activations import *\n",
    "from keras.metrics import mean_squared_error as mse\n",
    "from keras import backend as keras \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "from mriPrep import Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Definition of the Network Class, including loss function definition and convolution definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet:\n",
    "\n",
    "    # initialization, including declaration of input sizes, coefficients for loss function\n",
    "    def __init__(self):\n",
    "        self.input_size = (128, 128, 1)\n",
    "        self.coe = [random(), random()]\n",
    "\n",
    "    # compound loss function\n",
    "    def comp_loss(self):\n",
    "        ssim = 1 - tf.reduce_mean(tf.image.ssim(self.y_true, self.y_pred, 1.0))\n",
    "        compound_loss = (self.coe[0] * ssim) + (self.coe[1] * mse(self.y_true, self.y_pred)) \n",
    "        return compound_loss\n",
    "    \n",
    "    # side convolution\n",
    "    def convolution(self, inp, n_filters):\n",
    "        conv = Conv2D(n_filters, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\")(inp)\n",
    "        bn = BatchNormalization(axis=1, momentum=0.99, epsilon=0.0001)(conv)\n",
    "        return bn\n",
    "\n",
    "    # down convolution\n",
    "    def down_convolution(self, inp, n_filters):\n",
    "        conv = Conv2D(n_filters, (4, 4), activation=\"relu\", padding=\"same\", strides=(2,2) kernel_initializer=\"he_normal\")(inp)\n",
    "        bn = bn = BatchNormalization(axis=1, momentum=0.99, epsilon=0.0001)(conv)\n",
    "        return bn\n",
    "\n",
    "    # up convolution\n",
    "    def up_convolution(self, inp, n_filters, conv_features):\n",
    "        deconvolution = Conv2DTranspose(n_filters, (4, 4), activation = 'relu', padding = 'same', strides=(0.5, 0.5), kernel_initializer = 'he_normal')(inp)\n",
    "        concatenation = Concatenate([conv_features, deconvolution], axis=3)\n",
    "        return concatenation \n",
    "\n",
    "    # definition of neural network\n",
    "    def unet(self, pretrained_weights=None):\n",
    "\n",
    "        inputs = Input(self.input_size)\n",
    "        \n",
    "        # side\n",
    "        conv1 = self.convolution(inputs, 16)\n",
    "\n",
    "        # down\n",
    "        conv2 = self.down_convolution(conv1, 16)\n",
    "        \n",
    "        # side\n",
    "        conv3 = self.convolution(conv2, 32)\n",
    "        \n",
    "        # down\n",
    "        conv4 = self.down_convolution(conv3, 32)\n",
    "\n",
    "        # side\n",
    "        conv5 = self.convolution(conv4, 64)\n",
    "\n",
    "        # down\n",
    "        conv6 = self.down_convolution(conv5, 64)\n",
    "\n",
    "        # side\n",
    "        conv7 = self.convolution(conv6, 128)\n",
    "\n",
    "        # down \n",
    "        conv8 = self.down_convolution(conv7, 128)\n",
    "        \n",
    "        # side\n",
    "        conv9 = self.convolution(conv8, 256)\n",
    "\n",
    "        # up and merge\n",
    "        conv10 = self.up_convolution(conv9, 128, conv7)\n",
    "\n",
    "        # side\n",
    "        conv11 = self.convolution(conv10, 128)\n",
    "\n",
    "        # up and merge\n",
    "        conv12 = self.up_convolution(conv11, 64, conv5)\n",
    "\n",
    "        # side \n",
    "        conv13 = self.convolution(conv12, 64)\n",
    "\n",
    "        # up and merge\n",
    "        conv14 = self.up_convolution(conv13, 32, conv3)\n",
    "\n",
    "        # side \n",
    "        conv15 = self.convolution(conv14, 32)\n",
    "\n",
    "        # up and merge\n",
    "        conv16 = self.up_convolution(conv15, 16, conv1)\n",
    "\n",
    "        # side and merge \n",
    "        conv17 = self.convolution(conv16, 16)\n",
    "        merge = Concatenate([inputs, conv17], axis=3)\n",
    "\n",
    "        # final side\n",
    "        conv18 = Conv2D(1, 1, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\")(merge)\n",
    "\n",
    "        model = Model(input = inputs, output = conv18)\n",
    "        model.compile(optimizer= Adam(lr = 1e-4),loss=self.comp_loss,metrics=[\"accuracy\"])\n",
    "        model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Data, running preprocessing programs, and Splitting into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'ImageFileWriter_Execute'.\n  Possible C/C++ prototypes are:\n    itk::simple::ImageFileWriter::Execute(itk::simple::Image const &)\n    itk::simple::ImageFileWriter::Execute(itk::simple::Image const &,std::string const &,bool,int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m preprocessing \u001b[39m=\u001b[39m Preprocessing(\u001b[39m\"\u001b[39m\u001b[39m./data/modified/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m./data/preprocessed/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m preprocessing\u001b[39m.\u001b[39;49mextractBrain()\n",
      "File \u001b[0;32m~/Projects/Harmonizing-MRI-Scans/mriPrep.py:58\u001b[0m, in \u001b[0;36mPreprocessing.extractBrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m stripped, mask \u001b[39m=\u001b[39m robex(image)\n\u001b[1;32m     57\u001b[0m \u001b[39m# modified_path = self.removeExtension(input_image_path)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m sitk\u001b[39m.\u001b[39;49mWriteImage(stripped, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/preprocessed/scanner2_sub-CC110069_T1w_preprocessed.nii\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     59\u001b[0m os\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39mbixed.nii\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/SimpleITK/extra.py:407\u001b[0m, in \u001b[0;36mWriteImage\u001b[0;34m(image, fileName, useCompression, compressionLevel, imageIO, compressor)\u001b[0m\n\u001b[1;32m    405\u001b[0m writer\u001b[39m.\u001b[39mSetImageIO(imageIO)\n\u001b[1;32m    406\u001b[0m writer\u001b[39m.\u001b[39mSetCompressor(compressor)\n\u001b[0;32m--> 407\u001b[0m \u001b[39mreturn\u001b[39;00m writer\u001b[39m.\u001b[39;49mExecute(image)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/SimpleITK/SimpleITK.py:7913\u001b[0m, in \u001b[0;36mImageFileWriter.Execute\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   7906\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mExecute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m   7907\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   7908\u001b[0m \u001b[39m    Execute(ImageFileWriter self, Image arg2)\u001b[39;00m\n\u001b[1;32m   7909\u001b[0m \u001b[39m    Execute(ImageFileWriter self, Image arg2, std::string const & inFileName, bool useCompression, int compressionLevel)\u001b[39;00m\n\u001b[1;32m   7910\u001b[0m \n\u001b[1;32m   7911\u001b[0m \n\u001b[1;32m   7912\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 7913\u001b[0m     \u001b[39mreturn\u001b[39;00m _SimpleITK\u001b[39m.\u001b[39;49mImageFileWriter_Execute(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'ImageFileWriter_Execute'.\n  Possible C/C++ prototypes are:\n    itk::simple::ImageFileWriter::Execute(itk::simple::Image const &)\n    itk::simple::ImageFileWriter::Execute(itk::simple::Image const &,std::string const &,bool,int)\n"
     ]
    }
   ],
   "source": [
    "preprocessing = Preprocessing(\"./data/modified/\", \"./data/preprocessed/\")\n",
    "preprocessing.extractBrain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 20\n",
    "model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
